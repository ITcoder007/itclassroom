package com.xg.itclassroom.algorithm.fenzhi;

public class MapReduce {

	// 海量数据处理： 从100亿数据找到最大的 1w个数据 （top K问题）
	//                 n                m 
	/**

算法界称之为： topK问题 【搜索引擎最热的10个词】 【歌曲库下载量最高的10首歌】
一般思路： 分治 - trie树/hash - 小根堆
	a.将数据集按照哈希算法分解为多个小数据集
	b.使用trie树或者hash统计每个词频，使用小根堆求出topK
	c.在所有topK求出最终 topk

1.  核心算法： 使用1w个数据建 小根堆； 遍历剩余数据，若大于根元素则取而代之 并调整堆结构使之仍为小根堆 —— 建堆： mlog(m) ; 算法: nlog(m) ???
2. 优化： 分组存放 —— 每组找出最优解 —— 合并到一起得到最终结果

几种实际情况：
	1. 数据量小： 在内存中 快速排序 即可
			2. 小根堆 + 遍历

	a. 单机+大内存
		直接内存排序
	b. 单机多核+受限内存
		可开c个线程 —— 将数据分为 c*x个区域【文件】；
		每个线程处理完成之后，再处理下一个分区；
		由最后一个线程归并
	c. 单机单核+受限内存
		先数据分割，再按照a.操作
	d. 多机 + 受限内存
		MapReduce
		扫描数据找出最小、最大数值，划分区间扫描数据 —— 数据量小之后，再内存排序
	 */
	public static void main(String[] args) {

	}

}
